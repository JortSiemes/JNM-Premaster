Journalism & Mass Communication Quarterly 2016, Vol. 93(1) 59–79 © 2015 AEJMC Reprints and permissions: sagepub.com/journalsPermissions.nav DOI: 10.1177/1077699015606057 jmcq.sagepub.com
Interactivity and Online Credibility
Measuring Message Credibility: Construction and Validation of an Exclusive Scale
Alyssa Appelman1 and S. Shyam Sundar2
Abstract
Despite calls to conceptualize credibility as three separate concepts—source credibility, message credibility, and media credibility—there exists no scale that exclusively measures message credibility. To address this gap, the current study constructs and validates a new scale. Results from a confirmatory factor analysis suggest that message credibility, specifically in the context of news, can be measured by asking participants to rate how well three adjectives describe content: accurate, authentic, and believable. Validity and reliability tests are reported, and contributions to credibility research are discussed.
Keywords
credibility, message credibility, news media, journalism, research methods, scale construction, measurement, formative indicators, reflective indicators, confirmatory factor analysis (CFA)
Credibility is a significant area of research in persuasion and communication research. However, it is sometimes seen as purely a source attribute (i.e., the speaker is credible). Scholars have pointed out that credibility of a media message may be influenced by non-source factors, such as the medium or channel of delivery and even the structure of the messages themselves (e.g., Metzger, Flanagin, Eyal, Lemus, & Mccann, 2003), leading to calls for a separation of the concept by situating it at three different loci: source, medium, and message. Whereas considerable research and measurement tools exist for the first two (e.g., Chung, Nam, & Stefanone, 2012; Kohring & Matthes,
1Northern Kentucky University, Highland Heights, KY, USA 2The Pennsylvania State University, University Park, PA, USA
Corresponding Author:
S. Shyam Sundar, The Pennsylvania State University, 122 Carnegie Building, University Park, PA 16802, USA. Email: sss12@psu.edu
6 0 6 0 5 7 JMQXXX10.1177/1077699015606057Journalism & Mass Communication QuarterlyAppelman and Sundar
research-article2015


60 Journalism & Mass Communication Quarterly 93(1)
2005), the last mentioned is under-explicated in the literature. We have no exclusive scale for measuring message credibility. Such a scale would have significant use for multiple fields but would be particularly useful in studies of journalism. Currently, the only option for media scholars wanting to measure the credibility of a message is to use scales of source credibility or medium credibility (e.g., Dochterman & Stamp, 2010) or more general scales of media credibility (e.g., Sundar, 1999) because there are no scales for directly and exclusively measuring message credibility. In the current media environment, where we receive a plethora of messages, often without clear source signals or medium-specific cues (Sundar & Nass, 2001), a scale that exclusively measures message credibility would be useful in multiple situations. In particular, researchers in the following areas could benefit from an exclusive measure of message credibility:
1. Message Effects: Media scholars often manipulate elements of a news message (e.g., through word choice or framing) to see the effect of those message factors on credibility perceptions (e.g., Borah, 2013; Cole & Greer, 2013). This scale would allow for a systematic test of that effect without conflating message credibility with medium credibility or source credibility. 2. Information Processing: As Lang (2000) notes, we have a limited capacity for processing mediated messages. Information overload from these new technologies means we have insufficient bandwidth for storing information about both the source and the message. In other words, people could be considering the message without thinking about the source, and vice versa. A message credibility scale would help researchers access that distinction and quantify it. 3. Source Confusion: Digital messages are often repeated by multiple sources (e.g., by retweeting or reposting) or across multiple outlets (e.g., reposting a Facebook post on a personal blog). Therefore, readers do not always know the original source of the information. A scale such as this could allow scholars to examine perceptions of the message, regardless of the source or medium of transmission. 4. Social Media: Social media outlets, including Facebook and Twitter, have been the subject of several credibility studies. However, such studies focus on the perceived credibility of the person who posted the information (e.g., Cunningham & Bright, 2012; Edwards, Spence, Gentile, Edwards, & Edwards, 2013; Hwang, 2013; Park, Xiang, Josiam, & Kim, 2014) or perceived credibility of the social media site itself (e.g., Lee & Ahn, 2013; Schmierbach & OeldorfHirsch, 2012); they tend not to examine the perceived credibility of the social media content directly (e.g., Castillo, Mendoza, & Poblete, 2013). A message credibility scale, such as the one proposed here, creates possibilities for future research to examine how the content of the message is perceived.
As these cases illustrate, disambiguating message credibility from source credibility and medium credibility can enhance the clarity and quality of research in a number of theoretical and practical domains. This would require a scale specifically designed to


Appelman and Sundar 61
measure message credibility without conflating it with assessments of credibility that are attributable to source and/or medium. We propose and test one such scale. In preparation of this scale, we offer an exclusive definition and a unique measure for message credibility; through meaning analysis and confirmatory factor analyses (CFAs), we provide a parsimonious set of subconcepts that apply purely to message credibility.
Literature Review
The study of credibility, in general, and media credibility, in particular, has suffered from a basic lack of cohesion among academic researchers. This has resulted in two specific problems: lack of a clear definition and lack of a clear method for measuring credibility at different levels.
Media Credibility
This study examines media credibility specifically, rather than credibility in general. Carter and Greenberg (1965) were among the first to study media credibility as a unique concept. Judgments about media credibility are different than judgments about general credibility because of the potential for various dimensions of media to be confounded. Perloff (2010) writes, “(Credibility) is an audience member’s perceptions of the communicator’s qualities” (p. 166). Mediated communication is complicated because the source, message, and medium are often difficult to separate. Mass-mediated communication is embedded in its medium. For example, if one reads a newspaper article about a robbery, then one might make general credibility judgments based on his or her perception of the newspaper itself, the reporter who wrote that article, the publisher of the newspaper, the editor of the newspaper, or the section of the paper in which this story is published. The same piece of mediated communication published in a different media outlet would necessarily be judged differently, even if it were attributed to the same reporter and contained the same message. Media credibility, therefore, needs to be defined and measured differently than general credibility; it needs to consider the potential for the credibility of source, medium, and message to be confounded.
Definitions. Several studies define media credibility based on its characteristics. For example, it has been defined as “a global evaluation of the objectivity of the story” (Sundar, 1999, p. 380) and “a perceived quality based on multiple factors, including trustworthiness and expertise” (Chung et al., 2012, p. 173). Although these definitions are useful, it can be problematic to define a concept based on its subconcepts; in doing so, it becomes difficult to parse out whether the defining term is a synonym or a component. Other studies point to an even graver issue—that of not providing a definition at all (e.g., Borah, 2013; Castillo et al., 2013; Gaziano & McGrath, 1986; Hovland & Weiss, 1951).
Measurement. The second problem is the lack of a method of measurement. Different measures of media credibility exist for different types of media outlets. Traditional


62 Journalism & Mass Communication Quarterly 93(1)
measures of media credibility (e.g., fair, objective) do not take into account factors that could affect judgments of online media, so, as a result, researchers have created medium-specific measures (e.g., online media credibility: interactivity, multimediality, hypertextuality; Chung et al., 2012; blog credibility: authenticity, timeliness, popularity; Kang & Yang, 2011; web credibility: authority, page layout, site motive, URL, crosscheckability, user motive, content, date, professionalism, site familiarity, process, personal beliefs; Dochterman & Stamp, 2010). These measures, although useful, do not specifically address the credibility of the message.
Message Credibility Explication
The exercise of explicating message credibility has significant implications for both researchers and journalists. Explications are useful for researchers, generally, because they ensure that researchers are speaking the same language and using terms in the same way. In this way, an explication is needed as a means of best consolidating and advancing research; when researchers are using different definitions and methods, they lose the ability to compare findings and advance research (Chaffee, 1991). More particularly, an explication of message credibility has implications for journalists. Journalism, as an industry, only functions when people trust its content. With the proliferation of media outlets, both online and offline, people can go to any number of sources for information. Journalists, therefore, live in fear of losing their credibility and, by extension, their media audience. A better understanding of message credibility, then, can have implications for journalists and the work they produce.
Concepts similar to message credibility. Message credibility, as a subset of media credibility, can be situated vertically between two sets of concepts. The superordinate concepts, or concepts that come above message credibility, come from two strands of research. The general credibility literature suggests that credibility, or ethos, comes under the general concept of modes of persuasion, as conceptualized by Aristotle (McCroskey, 1966). The media-specific literature, however, suggests it comes under the general concept of news perceptions (Sundar, 1999). For the purposes of this study, message credibility will be seen as a concept below news perceptions. The subordinate concepts, or concepts that come below message credibility, are not clearly identified. Most explication research in this area attempts to identify subconcepts of media credibility, but these usually come in the form of source or medium credibility (e.g., Carter & Greenberg, 1965), rather than message credibility. McCroskey and Teven (1999), for example, suggest that credibility, in general, comes above competence, trustworthiness, and goodwill. Sundar (1999) suggests that media credibility is associated with perceptions of bias, fairness, objectivity, accuracy, and believability. Gaziano and McGrath (1986) provide a longer list of subcomponents: fair, not biased, tells whole story, accurate, respects people’s privacy, watches after readers’/viewers’ interests, concerned about community’s well-being, separates fact and opinion, can be trusted, concerned with public interest, is factual, and has well-trained reporters.


Appelman and Sundar 63
Message credibility can also be situated horizontally. The conceptualizations of sibling concepts, again, come from two strands of research. The general credibility literature suggests that media credibility is at the same level of abstraction as Aristotle’s other modes of persuasion: pathos and logos (McCroskey, 1966). The media-specific literature, however, suggests it is at the same level of abstraction as such concepts as trust (Kohring & Matthes, 2005), quality, liking, and representativeness (Sundar, 1999). Still other studies place these same concepts as subordinate concepts, rather than concepts at the same level of abstraction.
Characteristics of message credibility. Message credibility can be analyzed as a state, rather than as a structure or process. Previous studies have measured credibility judgments at a particular moment. In addition, message credibility can be analyzed as an effect, rather than as a cause or mediator. People’s perceptions of message credibility could, presumably, affect the way they make subsequent judgments, but it would still be an effect in its own right. The unit of analysis for studies of message credibility is the individual. Studies have repeatedly shown that people who see the same message can judge its credibility differently (e.g., Hovland & Weiss, 1951). In this way, then, perceptions can be measured person-by-person. Furthermore, research suggests that these individual perceptions of credibility can be based on social cues. Sundar (2008) outlines a model by which technological affordances of media (i.e., modality, agency, interactivity, and navigability) cue cognitive heuristics that affect credibility assessments. For example, when a friend emails me a news story, then I might see that friend as the source of the story (e.g., agency). That could cue a mental shortcut such as the “bandwagon heuristic” (i.e., other people like the article, so I should, too), which could make me think the article is popular, leading me to perceive the article as credible. Sundar (2008) outlines several other heuristics and qualities that could affect credibility judgments. This model has been tested by several researchers, including Metzger, Flanagin, and Medders (2010), who found that participants made credibility judgments with the help of social cues and information sharing. Specifically, they suggest five heuristics used in making media credibility judgments: reputation, endorsement, consistency, expectancy violation, and persuasive intent. Such heuristics could be part of a new measure of message credibility.
New definition of message credibility. As mentioned above, credibility studies have often defined credibility by its own components (e.g., Chung et al., 2012; Sundar, 1999) or neglected to provide a definition at all (e.g., Gaziano & McGrath, 1986; Hovland & Weiss, 1951). This study, therefore, poses a new definition of message credibility in the context of news obtained from media: Message credibility is an individual’s judgment of the veracity of the content of communication.
Our next task is to arrive at an exclusive measure of this perception.
Message Credibility Scale Development
Based on best practices for CFAs, this study begins by proposing a model. As discussed by Holbert and Stephenson (2008),


64 Journalism & Mass Communication Quarterly 93(1)
With CFA, a researcher posits a specific number of articulated factors in the form of latent variables, and a theory-driven set of associations between multiple observable variables and latent variables is outlined prior to any formal analyses. Thus, restrictions are being placed on the analysis of measurement in a CFA model. (p. 187)
Because the study’s purpose is to determine the best indicators of message credibility, we began by searching for previous measures of message credibility. However, no message-specific scales were found, so this study compiled all of the media credibility measures that seemed plausible in the context of messages. In addition, we gathered measures from a pretest comprising two focus groups (N1 = 5, N2 = 7) of undergraduate students. We began with a preset list of 13 directive and non-directive questions (e.g., How do you know whether you can believe something you read in the news?) and asked follow-up questions as needed. Analysis of the focus group transcriptions led us to several additional potential measures of message credibility (e.g., equal, error-free). Along with the media credibility measures we had found, this gave us a list of 75 items, including such varied items as morality (McCroskey & Teven, 1999), passion (Kang & Yang, 2011), and compatibility (Sundar, 2008). This was subjected to a meaning analysis by the authors, informed by a series of group discussions with about a dozen graduate students in mass communication. After several iterations, the potential list was narrowed down to 31 items and further divided into two groups: formative indicators, or factors that seemingly contribute to message credibility, and reflective indicators, or factors that seemingly reflect message credibility (Kline, 2011). Based on literature reviews and discussions, the formative indicators were hypothesized to include measures of quality (clear, complete, comprehensive, concise, consistent, detailed, transparent, well-presented, and well-written), measures of expertise (endorsed, expert, will have impact, and professional), and measures of fairness (balanced, not biased, equal, fair, neutral, objective, not opinionated, representative, and spin-free). Table 1 lists these 22 proposed formative indicators and their sources. Reflective indicators were hypothesized to include accurate, error-free, true, authentic, believable, reliable, authoritative, reputable, and trustworthy. Some of these terms—reliable, authoritative, reputable, trustworthy—could be considered to reflect source credibility more strongly than message credibility. Reputability, for example, could refer to the perceived reputation of the source or the perceived reputation of the message. Because of this potential overlap and the lack of a previous messagebased scale, they were included here to determine whether they reflect message credibility. Table 2 lists these nine proposed reflective indicators and their sources. The proposed factor structure of message credibility, then, has nine items that reflect the concept (i.e., reflective indicators) and 22 indicators that form it (i.e., formative indicators). This proposed model, although comprehensive, would result in a 31-item measure, which, due to participant fatigue, would not be feasible for research use, regardless of model fit. Thus, the purpose of this study is to analyze the proposed factor structure and to narrow it down into its most parsimonious form. A simplified scale of message credibility would give media scholars a feasible measure for use in everyday research on message credibility.


Appelman and Sundar 65
Although the model is adjusted based on model fit, the study is still a CFA, in the sense that it is theory-driven. “EFA is comparatively crude in that the analysis is driving the formation of the articulated factors (i.e., latent variables), whereas CFA is driven
Table 1. Hypothesized Formative Indicators of Message Credibility.
Indicator Item Citation
Quality Clear Sundar (1999, 2008) Complete Sundar (2008) Comprehensive Sundar (1999) Concise Sundar (1999, 2008) Consistent Kang and Yang (2011), Metzger, Flanagin, and Medders (2010), Sundar (2008) Detailed Sundar (2008) Transparent Kang and Yang (2011) Well-presented Modified from “Attractive,” Hovland, Janis, and Kelley (1953) Well-written Sundar (1999) Expertise Endorsed Metzger et al. (2010) Expert Hovland, Janis, and Kelley (1953), Sundar (2008) Will have impact Kang and Yang (2011) Professional Dochterman and Stamp (2010) Fairness Balanced Pretest Not Biased Sundar (1999) Equal Pretest
Fair Sundar (1999), Kang and Yang (2011) Neutral Pretest Objective Sundar (1999, 2008) Not opinionated Pretest Representative Sundar (2008) Spin-free Pretest
Table 2. Hypothesized Reflective Indicators of Message Credibility.
Indicator Item Citation
Message Credibility Accurate Sundar (1999), Kang and Yang (2011) Error-free Pretest True Pretest
Authentic Kang and Yang (2011) Believable Sundar (2008) Reliable Kang and Yang (2011), Sundar (2008) Authoritative Dochterman and Stamp (2010) Reputable Metzger, Flanagin, and Medders (2010) Trustworthy Sundar (2008)


66 Journalism & Mass Communication Quarterly 93(1)
foremost by the theoretical claims of the researcher” (Holbert & Stephenson, 2008, p. 187). Overall, this study confirms the theoretically driven model while narrowing it down into its most parsimonious, user-friendly version. Rather than exploring an underlying structure, as in an exploratory factor analysis (EFA), we are interested in confirming the relationship pattern predicted by our meaning analysis. This would necessitate a CFA (e.g., Devellis, 2012). To test the scale’s convergent validity, results of the new measure were compared with journalists’ credibility judgments to assess whether scores on the measures were similar. The credibility measure was also compared with the related constructs of liking and newsworthiness. To test the scale’s discriminant validity, two models were compared with one another: one in which the scale is the same as these related variables (i.e., the correlation between credibility and liking as well as that between credibility and newsworthiness was constrained to be 1) and one in which the variables were different (i.e., the correlation was unconstrained). These methods of validity testing have been used in previous studies (e.g., Hayes, Glynn, & Shanahan, 2005; Shen, Condit, & Wright, 2008; Wang & Senecal, 2007). To summarize, this study addresses the following research questions:
RQ1: What are the best formative and reflective indicators of message credibility? RQ2: Is the proposed measure of message credibility valid and reliable?
Method
A within-subjects experiment was conducted to test the validity and reliability of the proposed message credibility measure. Participants read two news articles—one high in credibility and one low in credibility—and rated them based on the proposed scale items.
Participants
Participants (N = 322) were recruited from Amazon Mechanical Turk (MTurk) and compensated 50 cents each for their participation. MTurk allows participants (i.e., “workers”) to complete studies (i.e., “tasks”) created by researchers (i.e., “requesters”). This online outlet has become popular for social scientists because it provides large samples of motivated participants at low cost (Paolacci & Chandler, 2014). Understandably, questions have arisen about the representativeness of such samples. Demographic analysis, however, shows that, although not representative of the U.S. population,
Mechanical Turk workers are at least as representative of the U.S. population as traditional subject pools, with gender, race, age and education of Internet samples all matching the population more closely than college undergraduate samples and internet samples in general. (Paolacci, Chandler, & Ipeirotis, 2010, p. 414)


Appelman and Sundar 67
In line with this analysis, our study’s sample was as diverse as, if not more diverse than, the average undergraduate sample. Gender was almost evenly split: 187 males, 134 females. Ages ranged from 18 to 76 (M = 34.87, SD = 11.91), which is more comprehensive than the average undergraduate sample. In addition, participants represented geographic diversity within the United States, which is usually not possible with university-based studies. The sample was mostly White (n = 272), although some participants were Black (n = 14), Hispanic (n = 12), Asian (n = 17), and selfidentified as Other (n = 7). In addition, we argue that the nature of the study makes an MTurk sample all the more appropriate. Considering that we are measuring perceptions of news stories presented online and given the increasing reliance on online media for obtaining news, this study is best served by a sample of participants who are not only adept at online media but also frequent users of the Internet.
Design
This study was set up as a within-subjects experiment. Participants each read two news articles (one high credibility and one low credibility) and answered questions about their perceptions of those articles. They also provided demographic information and information about their news consumption. Whereas scale construction studies are often conducted in the form of surveys (e.g., Brown, 2003; Shen et al., 2008), it is also appropriate to use an experimental design as a foundation for analysis (e.g., Wang & Senecal, 2007). For this study, in particular, we wanted to determine how well the measures reflected the underlying concept of message credibility, not just how consistent they were with each other. Streiner and Kottner (2014) refer to these as validity studies rather than reliability studies: “Validity studies are tests of hypotheses; e.g. scores will differ between different groups, or will change in response to some intervention, or are correlated (or not) with scores on another test” (p. 1973). With these studies, they explain, the methods are more varied:
Some studies correlate the new scale with an existing one or other clinical parameters; others look at whether scores are different following some intervention (using a paired t-test) or in groups that should be high and low on the construct that is tapped by the scale (independent t-test); or compare groups that should and should not change in response to some treatment (repeated measures ANOVA or ANCOVA); and so forth. (p. 1975)
Thus, although less common, the experimental method is apt for this type of validity test.
Stimulus
Several types of messages could be analyzed within this framework. Because the ultimate goal of this study is to provide a resource for media scholars, this study used news articles as the stimulus messages.


68 Journalism & Mass Communication Quarterly 93(1)
Pretest. This study began with a pretest to confirm the stimulus article selection and content clarity. The researchers sent 10 online news articles, without source or medium information, to professional journalists through an online survey system. The articles originated from various online news sites (e.g., NPR, National Enquirer) and differed in terms of topic, sourcing, and formality of tone. They were presented to the journalists as textual articles without source cues or photos. The participants were given the following explanation of the pretest:
The purpose of my complete study will be to test a new measure of media credibility, but first I need to make sure I have credible and non-credible news articles to use as stimulus material for my experiment. Please read the following 10 stories and identify how credible you think each is. Please do not use the Internet to look up the story or additional information; just rate the story on its own.
The 10 articles were presented to them one at a time. After reading each article, participants were asked to respond to one open-ended question: How credible was the article you just read (not at all credible, somewhat credible, credible, or very credible), and why? The article order was randomized for each participant. Four journalists responded. Based on their feedback, three articles were unanimously considered to be highly credible, and three were unanimously considered to be non-credible. The other four articles received less consistent feedback and were, therefore, removed from the study. A second pretest of five participants confirmed that participants could read one article and answer its related questions within 5 min. The articles were all relatively short: 250 to 500 words.
Experiment. Six articles were used in the final study: three that were deemed highly credible by the journalists and three that were deemed non-credible. Each participant was randomly assigned to read one of the high-credibility articles and one of the lowcredibility articles, and the order in which they saw these was randomized. Some saw the low-credibility article before the high-credibility article (n = 169), and the rest saw the high-credibility article before the low-credibility article (n = 153). The participants were fairly evenly divided across the six articles, as well (n = 101, n = 108, n = 113, n = 103, n = 114, n = 105). The six articles represented different topics, as well as the different levels of credibility. The articles were presented online and were formatted to look like news articles. They were given large headlines, a “Staff Report” byline, and a dateline. However, no source was identified so that participants could focus on the message. Participants read one article and then responded to the list of adjectives on the next page; then they read the next article, and responded to the list of adjectives on the next page. They were unable to go back and forth between the text and the questions.
Measures
Participants were asked to indicate how well several adjectives represent the article they just read, from 1 = describes very poorly to 7 = describes very well. These


Appelman and Sundar 69
adjectives included the proposed formative measures of message credibility (clear, complete, comprehensive, concise, consistent, detailed, transparent, well-presented, well-written, endorsed, expert, will have impact, professional, balanced, biased, equal, fair, neutral, objective, opinionated, representative, and spin-free) and the proposed reflective measures of message credibility (accurate, error-free, true, authentic, believable, reliable, authoritative, reputable, and trustworthy). Some were presented in the negative for the sake of variety and rigor. Participants were also asked to indicate the applicability of adjectives that reflect related scales of newsworthiness (informative, important, serious, and disturbing; Mayo & Leshner, 2000) and liking (boring, enjoyable, lively, and pleasing; Sundar, 1999). This set of measures was also measured from 1 = describes very poorly to 7 = describes very well.
The adjectives for all of these scales were randomized across participants to prevent any order effects. Finally, participants indicated how often they consumed news media from different outlets from 1 = never to 7 = daily. Participants mostly consumed online news (M = 5.97, SD = 1.55); however, they also reported frequent use of network television (M = 4.00, SD = 2.20) and cable television (M = 3.83, SD = 2.24). Participants also reported moderate levels of news consumption via radio (M = 3.46, SD = 2.18), independent blogs (M = 3.53, SD = 2.18), blogs owned by news outlets (M = 3.45, SD = 2.15), local newspapers (M = 3.43, SD = 2.06), and national newspapers (M = 2.81, SD = 1.99).
Results
RQ1: What are the formative and reflective indicators of message credibility?
Through a series of CFAs, this study sought to determine a parsimonious factor structure of message credibility. First, the reflective indicators were considered through a lower order factor model, which assessed the relationships between the nine proposed reflective measures of message credibility. No clear pattern emerged in terms of ways to group these adjectives. Thus, all nine factors were kept as reflective indicators. As is customary in formative measurement models, indicators covaried with each other (e.g., Bledow & Frese, 2009; Brown, n.d.; Diamantopoulos & Winklhofer, 2001; Helm, 2005). When we checked for data normality, we found no skewness or kurtosis concerns with the individual measures; no skewness measures were greater than 3 or less than −3, and no kurtosis measures were greater than 10 or less than −10. In addition, the multivariate model was normal. According to Bollen (1989), if Mardia’s coefficient is less than P(P + 2), where P is the number of observed variables, this indicates multivariate normality. For this study, Mardia’s coefficient was 47.165, which is much less than P(P + 2)—which, in this case, is 70 × 72—so the model itself was considered normal, as well. A model was, thus, constructed in which quality and fairness formed message credibility, which was reflected in nine indicators. According to best practices for structural equation modeling, good-fitting models should have a non-significant χ2, root mean square error of approximation (RMSEA)


70 Journalism & Mass Communication Quarterly 93(1)
below .08, comparative fit index (CFI) above .95, and standardized root mean square residual (SRMR) below .08 (Hu & Bentler, 1999). However, χ2-significance tests are sensitive to sample size. Hoe (2008) says this is especially true when the sample is greater than 200 observations, as in this study. He suggests using a ratio of χ2/df to combat this bias, where a value of 3 or less is considered acceptable. Based on these guidelines, the model fit of the initial model was quite close to acceptable on most indicators (χ2 = 693.777, df = 199, p < .001; χ2/df = 3.49; RMSEA = .088; 90% confidence interval [CI] = [.081, .095]; CFI = .926; and SRMR = .21). Closer examination revealed that several formative indicator paths were not significant. The 12 non-significant items were removed from the model (i.e., clear, comprehensive, detailed, transparent, well-written, balanced, not biased, equal, fair, neutral, not opinionated, and endorsed). Individual factor loadings of reflective indicators on the latent variable of message credibility were then examined, and all loadings that were lower than .70 (Bagozzi & Yi, 2012) were removed from the model (i.e., authoritative, trustworthy, reliable, true, and error-free). These measures all seem to reflect intentionality of the message source, rather than purely message-based factors, so in retrospect, their exclusion serves to more sharply operationalize message credibility around its core conceptualization. Again, the purpose of this analysis is to narrow down the omnibus model, so removal of items is a necessary step toward achieving the intended parsimony. As advised by Diamantopoulos and Winklhofer (2001), multicollinearity among the formative indicators was assessed. The maximum variance inflation factor was 2.841, which is well below the common cutoff of 10 (e.g., Diamantopoulos & Winklhofer, 2001; Helm, 2005). This resulted in a multiple indicators and multiple causes (MIMIC) model, which represents an identified model structure (e.g., Brown, n.d., Model B; and Diamantopoulos & Winklhofer, 2001, Figure 2). Error covariances were then added to the reflective indicators as suggested by modification indices. This model had an acceptable overall fit (χ2 = 68.801, df = 31, p = .000; χ2/df = 2.22; RMSEA = .062; 90% CI = [.042, .081]; CFI = .986; and SRMR = .0192). It appears that message credibility is best reflected in the following four indicators: accurate, authentic, believable, and reputable. However, the last indicator mentioned implies the involvement of source factors, considering that the reputation of a news story is likely to be largely derived from the credibility of the source of that news story. Some of the formative indicators (e.g., expert, professional) suggest that reputability may be a reflection of the veneer of professionalism associated with a news story, either in the way it is presented or the expertise underlying the production of that story. Given the potential involvement of source factors in receiver judgments of reputability of a message, and considering our central objective of developing a scale that exclusively captures message credibility, we tested a final model in which reputability was removed. Although the fit is not as good as the previous model, it is still acceptable (χ2 = 48.364, df = 19, p = .000; χ2/df = 2.55; RMSEA = .069; 90% CI = [.045, .094]; CFI = .987; and SRMR = .0161). This final model, shown in Figure 1, represents the best-fitting, face-valid factor structure of pure message credibility.


Appelman and Sundar 71
Based on this model, message credibility has 10 formative indicators (i.e., complete, concise, consistent, well-presented, objective, representative, no spin, expert, will have impact, and professional) and three reflective indicators (i.e., accurate, authentic, and believable). A three-factor solution means that message credibility can be modeled efficiently as a latent variable in structural equation modeling analyses, so this final measure is both useful and appropriate for use by media researchers and communication scholars. The three-item measure was then examined. Because the measures ranged from 1 to 7, each participant could be assigned a score of 3 to 21 for each article. Analyses show that participants thought the articles were in all fairly credible (M = 14.65, SD = 3.02). Message credibility measures differed across the three low-credibility articles (M = 14.80, SD = 3.69; M = 12.40, SD = 4.60; M = 13.42, SD = 3.78) and the three high-credibility articles (M = 15.58, SD = 3.29; M = 15.96, SD = 3.23; M = 15.82, SD = 3.32), which shows the measure’s variability. In all, the highcredibility articles were seen as significantly more credible (M = 15.80, SD = 3.27) than the low-credibility articles (M = 13.51, SD = 4.15), t(321) = 9.31, p < .001. The three items were then averaged together to form a scale (M = 4.88, SD = 1.01) and subjected to the following validity and reliability tests.
RQ2: Is the proposed measure of message credibility valid and reliable?
Several validity and reliability tests were run, as explained below. These include the three core measures of validity, as discussed by Devellis (2012) and Hayes et al. (2005): content validity, criterion validity, and construct validity. In addition, we examine two measures of reliability: item reliability and scale reliability.
Accurate
Authentic
Believable
e1
e2
e3
e4
.85
.84
.76
.14
.11 .20
.17
.08
.15
.15
.08 .07 .21
Consistent
Concise
Complete
Well-Presented
Objective
Representative
No Spin
Expert
Will Have Impact
Professional
X2 = 48.364, df = 19, p < .001; X2 / df = 2.55 RMSEA = .069, 90% CI = .045-.094, CFI = .987, SRMR = .0161
Message Credibility
.19
Figure 1. Final factor structure of message credibility. Note. RMSEA = root mean square error approximation; CI = confidence interval; CFI = comparative fit index; SRMR = standardized root mean square residual.


72 Journalism & Mass Communication Quarterly 93(1)
Content validity. First, consideration of message credibility as accurate, authentic, and believable is face valid. That is, it makes logical sense that the three items would reflect message credibility. The items are all message-based, rather than source- or medium-based, and they all logically contain the notion of credibility. Second, the scale appears to measure all dimensions of message credibility. All three measuresaccurate, authentic, and believable—make sense in the context of the proposed definition of message credibility: Message credibility is an individual’s judgment of the content of communication. This suggests high content validity.
Criterion validity. To measure criterion validity, the new measure was tested against the journalists’ credibility judgments to see whether scores on the measures were similar. A paired t test showed high-credibility articles (as determined by the journalists) were judged significantly more credible on this three-item scale (M = 5.27, SD = 1.09) than low-credibility articles (M = 4.50, SD = 1.38), t(321) = 9.318, p < .001.1 Although the actual difference in means might not seem meaningfully different (5.27 vs. 4.50), an effect-size calculation (Becker, 1998) shows a Cohen’s d of .619, which corresponds to a moderate to large effect. Thus, we see a significant and meaningful difference between the types of articles through our new measure. This means the credibility judgments made by journalists are shared by readers using our measures, which suggests high concurrent validity.
Construct validity. For a measure to possess good construct validity, it has to be statistically related to other constructs that are logically similar (convergent validity), without being identical to those other constructs (discriminant validity). We compared our credibility measure with other related constructs, including liking and newsworthiness. We also compared it with the source-related message measures that were removed from the proposed message credibility measure (i.e., authoritative, reliable, reputable, and trustworthy). The other two items removed from the proposed measure (i.e., errorfree and true) do not represent source and were, therefore, not included in this analysis. (Factor loadings and modification indices suggest they were highly correlated with accuracy, which is why they were not significant on their own.) One at a time, the latent variables and their indicators were entered into a model with the new message credibility measure. In all cases, message credibility was positively correlated with these related constructs. The correlation terms (i.e., φ) and model fits are shown in Table 3. These positive correlations suggest high convergent validity. Discriminant validity was established using the same variables from the convergent validity test. If message credibility is so highly correlated with these measures, one could argue, then why do we need a separate measure for message credibility? To determine the answer to this question and establish that our measure of message credibility is not redundant with liking, newsworthiness, and source-related measures, the following two models were compared: one in which the variables were treated as identical (by constraining the covariance between them to be 1) and one in which the variables were assumed to be different (i.e., the correlation was unconstrained). A significant difference in fit between these two types of models would suggest that


Appelman and Sundar 73
the variables are significantly different from each other. In all cases, message credibility was found to be distinct from these variables; that is, the models fit significantly better when message credibility and each of the other three variables were allowed to freely covary (two-factor model) than when they were assumed to be perfectly correlated (single-factor model). These results are also shown in Table 3 and suggest high discriminant validity. The measure was then considered in association with demographic variables, including sex and age. We did not find any effects.
Item and scale reliability. Communality estimates obtained by squaring the factor loadings seen in Figure 1 suggest that a significant portion of the variance of each of these three items is explained by the latent variable of message credibility. A scale reliability test suggests that this three-item measure is highly reliable as well (Cronbach’s α = .87).
Discussion
The major discovery of this study is that message credibility can be measured by asking participants to rate how well certain adjectives describe content: accurate, authentic, and believable. Together, these three adjectives reflect the concept of message credibility. This scale is not only reliable and valid, but it is also parsimonious and theorydriven. It thus serves as a useful measure for studies of message credibility, both within and outside the field of journalism. The analysis shows that this measure is related to, but statistically distinct from, measures of liking and newsworthiness. It is highly correlated with these variables, as
Table 3. Convergent and Discriminant Validity of the Three-Item Message Credibility Scale.
Convergent validity (φ with message credibility)
Single-factor model
Two-factor model
Discriminant validity (Δ from single- to two-factor model)
Liking .42 χ2 = 288.44 χ2 = 44.83 Δχ2 = 243.61 RMSEA = .25 RMSEA = .09 Δdf = 1 CFI = .69 CFI = .96 p < .001 SRMR = .15 SRMR = .06 Newsworthiness .89 χ2 = 93.16 χ2 = 63.24 Δχ2 = 29.92 RMSEA = .13 RMSEA = .11 Δdf = 1 CFI = .93 CFI = .96 p < .001 SRMR = .06 SRMR = .05 Source-related message measures
.75 χ2 = 196.15 RMSEA = .26 CFI = .85 SRMR = .09
χ2 = 180.01 RMSEA = .26 CFI = .86 SRMR = .12
Δχ2 = 16.14 Δdf = 1 p < .001
Note. RMSEA = root mean square error of approximation; CI = confidence interval; CFI = comparative fit index; SRMR = standardized root mean square residual.


74 Journalism & Mass Communication Quarterly 93(1)
expected, yet it is empirically distinct from them, as it should be, considering that they hold conceptually distinct meanings. Based on this model, message credibility is also related to, but distinct from, several source-related measures of message credibility: authoritative, reliable, reputable, and trustworthy. These concepts, although related to message credibility, have elements of source perceptions in them as well. Message credibility is distinct from source credibility, and our new measure reflects that difference. As shown in Figure 1, elements of writing quality, fairness, and professional expertise inform message credibility. This provides a unique and important contribution to the credibility literature; these concepts contribute to the notion of message credibility, but they do not measure it. Therefore, this study serves to distinguish between those measures that capture the core contributors of message credibility and those that reflect the concept. In doing so, it motivates a richer conceptualization of message credibility, with theoretical implications: From our model, it appears that professional writing quality (complete, concise, consistent, well-presented) contributes quite significantly to perceptions of message credibility, as does a sense of fairness. Items such as representative suggest the importance of achieving balanced coverage by representing multiple sides of an issue, whereas items such as objective and no spin underscore the need for impartiality on the part of the journalist. Moreover, an aura of professionalism is a significant predictor of message credibility. Perceived expertise of the story and its apparent sense of impact also appear to factor into user conceptions of message credibility. Whereas these characteristics (writing quality, balance, objectivity, professionalism, impact) are important contributors of message credibility, none of them can serve as a proxy for message credibility. For that, we turn to the reflective indicators: Perceived accuracy of a news story, its perceived authenticity, and its believability can each be equated with the perceived credibility of a news story, providing researchers with three nuanced views of the concept, as perceived by news consumers. These three aspects (accurate, authentic, and believable) serve to clarify the concept of message credibility by positioning it as being distinct from source credibility and media credibility, thereby making both a theoretical and a methodological contribution to the literature. It is worth noting that two of these three measures, accuracy and authenticity, could be considered to be more objective, whereas the third, believability, could be considered to be more subjective. Because the proposed measure is based on self-report perceptions, these measures are all, in fact, subjective. In other words, we could view the three indicators as perceived accuracy, perceived authenticity, and believability. In all, we can summarize the results of the study in the following manner: 10 indicators capture important aspects of message credibility (complete, concise, consistent, well-presented, objective, representative, no spin, expert, will have impact, professional), whereas three indicators best reflect message credibility (accurate, authentic, believable).
Theoretical and Practical Implications
Responding to calls by credibility researchers to treat message credibility as a distinct concept (e.g., Metzger et al., 2003), this study explicated it as being different from source credibility and medium credibility, providing a unique definition, which was


Appelman and Sundar 75
refined and operationalized in terms of scale items. These items tapped into individuals’ ratings of perceived message credibility in a reliable manner and showed convergent as well as discriminant validity. Although this study was conducted in an online context, we argue that its implications extend to print media as well. The study’s questions and its resulting scale had no medium-specific components; therefore, this scale can be used in message contexts across media. However, the sample used in this study, comprising micro-workers who are adept at online technology use, may not be representative of all media users, thus calling into question the generalizability of our findings.
Limitations and Future Research
Because this was an online study, rather than one conducted in a laboratory, it is unclear how closely the participants were paying attention or what other activities they were doing while reading the articles. Although it may be argued that the external validity gained from this method—where participants are reading online news articles the way they normally read them—outweighs internal validity concerns, it would be useful for a future study to test this measure in a more controlled setting. On a related note, we also did not account for the time and effort readers expended in reading each article. It could be that their perceptions of message credibility were influenced by not only the article content but also the amount of effort they exerted in reading it. A future study could control for processing time and effort to address this concern. In addition, the scale developed in this study is based on reader perceptions of a single type of messages, namely, news articles. This scale could potentially be applied to other messages, such as those used in advertising and strategic communications; however, it might need adaptation, especially in terms of eschewing elements of journalism. For example, the journalistic preoccupation with accuracy may not be relevant for ascertaining the credibility of persuasive messages. Future studies using this scale in non-journalistic contexts could determine the generalizability of this measure beyond the realm of news articles. Finally, because this study was a test of message credibility, both source and medium credibility cues were purposely kept constant. It could be that these different perceptions exert different effects on readers, and a future study that measured all of them could help us determine the relative effects of each. For example, message credibility may not be important when source credibility is high. On the other hand, if the readers perceive a message to be of low accuracy, authenticity, and believability, it could even hurt their evaluations of source credibility. This study was not set up to adequately test these possibilities, but future research on interaction effects of different types of credibility can certainly benefit by employing the exclusive message credibility scale developed in this study.
Conclusion
Based on this analysis, we can measure message credibility (“an individual’s judgment of the veracity of the content of communication”) by asking participants the following


76 Journalism & Mass Communication Quarterly 93(1)
question: How well do the following adjectives describe the content you just read? (from 1 = describes very poorly to 7 = describes very well): accurate, authentic, believable. This provides a parsimonious and usable metric for gauging credibility of messages for use in academic as well as industry research.
Acknowledgment
The authors thank anonymous reviewers at Journalism & Mass Communication Quarterly (JMCQ) and the International Communication Association, and members of Dr. Sundar’s lab group for their feedback on earlier versions of this project.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.
Funding
The author(s) received the following financial support for the research, authorship, and/or publication of this article: The first author was supported by a summer research grant awarded by the College of Communications at The Pennsylvania State University during her doctoral program.
Note
1. These means and standard deviations are based on the scale constructed as the average of the three items, from 1 to 7. As stated earlier, the message credibility score for the highcredibility articles on the additive, 3- to 21-point scale was M = 15.80, SD = 3.27, and the score for the low-credibility articles was M = 13.51, SD = 4.15.
References
Bagozzi, R., & Yi, Y. (2012). Specification, evaluation, and interpretation of structural equation models. Journal of the Academy of Marketing Science, 40, 8-34. doi:10.1007/ s11747-011-0278-x Becker, L. (1998). Effect size calculators. Retrieved from http://www.uccs.edu/~lbecker/ Bledow, R., & Frese, M. (2009). A situational judgment test of personal initiative and its relationship to performance. Personnel Psychology, 62, 229-258. doi:10.1111/j.17446570.2009.01137.x Bollen, K. A. (1989). Structural equations with latent variables. New York, NY: John Wiley. Borah, P. (2013). Interactions of news frames and incivility in the political blogosphere: Examining perceptual outcomes. Political Communication, 30, 456-473. doi:10.1080/10 584609.2012.737426 Brown, T. A. (2003). Confirmatory factor analysis of the Penn State Worry Questionnaire: Multiple factors or method effects? Behaviour Research and Therapy, 41, 1411-1426. doi:10.1016/S0005-7967(03)00059-7
Brown, T. A. (n.d.). Examples of unidentified and identified models with formative indicators. Retrieved from http://people.bu.edu/tabrown/Figure83.pdf Carter, R. F., & Greenberg, B. S. (1965). Newspapers or television: Which do you believe? Journalism&MassCommunicationQuarterly,42,29-34.doi:10.1177/107769906504200104


Appelman and Sundar 77
Castillo, C., Mendoza, M., & Poblete, B. (2013). Predicting information credibility in timesensitive social media. Internet Research, 23, 560-588. doi:10.1108/IntR-05-2012-0095 Chaffee, S. (1991). Communication concepts 1: Explication. Newbury Park, CA: SAGE. Chung, C., Nam, Y., & Stefanone, M. (2012). Exploring online news credibility: The relative influence of traditional and technological factors. Journal of Computer-Mediated Communication, 17, 171-186. doi:10.1111/j.1083-6101.2011.01565.x Cole, J. T., & Greer, J. D. (2013). Audience response to brand journalism: The effect of frame, source, and involvement. Journalism & Mass Communication Quarterly, 90, 673-690. doi:10.1177/1077699013503160 Cunningham, N., & Bright, L. F. (2012). The Tweet is in your court: Measuring attitude towards athlete endorsements in social media. International Journal of Integrated Marketing Communications, 4, 73-87.
Devellis, R. F. (2012). Scale development: Theory and application. Thousand Oaks, CA: SAGE. Diamantopoulos, A., & Winklhofer, H. (2001). Index construction with formative indicators: An alternative to scale development. Journal of Marketing Research, 38, 269-277. doi:10.1509/jmkr.38.2.269.18845 Dochterman, M., & Stamp, G. (2010). Part 1: The determination of web credibility: A thematic analysis of web user’s judgments. Qualitative Research Reports in Communication, 11, 37-43. doi:10.1080/17459430903514791 Edwards, C., Spence, P. R., Gentile, C. J., Edwards, A., & Edwards, A. (2013). How much Klout do you have . . . A test of system generated cues on source credibility. Computers in Human Behavior, 29(5), 12-16. doi:10.1016/j.chb.2012.12.034 Gaziano, C., & McGrath, K. (1986). Measuring the concept of credibility. Journalism Quarterly, 63, 451-462. Hayes, A. F., Glynn, C. J., & Shanahan, J. (2005). Willingness to self-censor: A construct and measurement tool for public opinion research. International Journal of Public Opinion Research, 17, 298-323. doi:10.1093/ijpor/edh073 Helm, S. (2005). Designing a formative measure for corporate reputation. Corporate Reputation Review, 8, 95-109. doi:10.1057/palgrave.crr.1540242 Hoe, S. L. (2008). Issues and procedures in adopting structural equation modeling technique. Journal of Applied Quantitative Methods, 3, 76-83.
Holbert, R. L., & Stephenson, M. T. (2008). Commentary on the uses and misuses of structural equation modeling in communication research. In A. F. Hayes, M. D. Slater, & L. B. Snyder (Eds.), The SAGE sourcebook of advanced data analysis methods for communication research (pp. 185-218). Thousand Oaks, CA: SAGE. Hovland, C., & Weiss, W. (1951). The influence of source credibility on communication effectiveness. Public Opinion Quarterly, 15, 635-650. doi:10.1086/266350 Hovland, C. I., Janis, I. L., & Kelley, H. H. (1953). Communication and Persuasion. New Haven, CT: Yale University Press. Hu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling, 6, 1-55. doi:10.1080/10705519909540118 Hwang, S. (2013). The effect of Twitter use on politicians’ credibility and attitudes toward politicians. Journal of Public Relations Research, 25, 246-258. doi:10.1080/10627 26X.2013.788445
Kang, M., & Yang, S. (2011, May). Measuring social media credibility: A study on a measure of blog credibility. Paper presented at the 61st annual conference of the International Communication Association, Boston, MA.


78 Journalism & Mass Communication Quarterly 93(1)
Kline, R. B. (2011). Principles and practice of structural equation modeling (3rd ed.). New York, NY: Guilford Press.
Kohring, M., & Matthes, J. (2005, May). Trust in news media. Development and validation of a multidimensional scale. Paper presented at the annual meeting of the International Communication Association, New York, NY. Lang, A. (2000). The limited capacity model of mediated message processing. Journal of Communication, 50, 46-70. doi:10.1111/j.1460-2466.2000.tb02833.x Lee, Y., & Ahn, H. (2013). Interaction effects of perceived sponsor motives and Facebook credibility on willingness to visit social cause Facebook page. Journal of Interactive Advertising, 13, 41-52. doi:10.1080/15252019.2013.768056 Mayo, J., & Leshner, G. (2000). Assessing the credibility of computer-assisted reporting. Newspaper Research Journal, 21(4), 68-82.
McCroskey, J. (1966). Scales for the measurement of ethos. Speech Monographs, 33, 65-72. doi:10.1080/03637756609375482 McCroskey, J., & Teven, J. (1999). Goodwill: A reexamination of the construct and its measurement. Communication Monographs, 66, 90-103. doi:10.1080/03637759909376464 Metzger, M. J., Flanagin, A. J., Eyal, K., Lemus, D., & Mccann, R. (2003). Credibility for the 21st century: Integrating perspectives on source, message, and media credibility in the contemporary media environment. Communication Yearbook, 27, 293-335. Metzger, M. J., Flanagin, A. J., & Medders, R. B. (2010). Social and heuristic approaches to credibility evaluation online. Journal of Communication, 60, 413-439. doi:10.1111/ j.1460-2466.2010.01488.x Paolacci, G., & Chandler, J. (2014). Inside the Turk: Understanding mechanical Turk as a participant pool. Current Directions in Psychological Science, 23, 184-188. doi:10.1177/0963721414531598 Paolacci, G., Chandler, J., & Ipeirotis, P. G. (2010). Running experiments on Amazon Mechanical Turk. Judgment and Decision Making, 5, 411-419.
Park, H., Xiang, Z., Josiam, B., & Kim, H. (2014). Personal profile information as cues of credibility in online travel reviews. Anatolia: An International Journal of Tourism and Hospitality Research, 25, 13-23. doi:10.1080/13032917.2013.820203
Perloff, R. (2010). The dynamics of persuasion: Communication and attitudes in the 21st century. New York, NY: Routledge. Schmierbach, M., & Oeldorf-Hirsch, A. (2012). A little bird told me, so I didn’t believe it: Twitter, credibility, and issue perceptions. Communication Quarterly, 60, 317-337. doi:10.1080/01463373.2012.688723 Shen, L., Condit, C., & Wright, L. (2008, May). The psychometric property and validation of a fatalism scale. Paper presented at the annual meeting of the International Communication Association, Montreal, Québec, Canada. Streiner, D. L., & Kottner, J. (2014). Recommendations for reporting the results of studies of instrument and scale development and testing. Journal of Advanced Nursing, 70, 1970-1979. doi:10.1111/jan.12402 Sundar, S. S. (1999). Exploring receivers’ criteria for perception of print and online news. Journalism & Mass Communication Quarterly, 76, 373-386. doi:10.1177/ 107769909907600213 Sundar, S. S. (2008). The MAIN model: A heuristic approach to understanding technology effects on credibility. In M. Metzger & A. Flanagin (Eds.), Digital media, youth, and credibility (pp. 73-100). Cambridge, MA: MIT Press.


Appelman and Sundar 79
Sundar, S. S., & Nass, C. (2001). Conceptualizing sources in online news. Journal of Communication, 51, 52-72. doi:10.1111/j.1460-2466.2001.tb02872.x Wang, J., & Senecal, S. (2007). Measuring perceived website usability. Journal of Internet Commerce, 6(4), 97-112. doi:10.1080/15332860802086318
Author Biographies
Alyssa Appelman is an assistant professor in the Department of Communication at Northern Kentucky University. Her research examines the cognitive and perceptual effects of journalistic style. She is particularly interested in the ways news conventions affect knowledge-gain and credibility perceptions.
S. Shyam Sundar is a distinguished professor and co-director of the Media Effects Research Laboratory in the College of Communications at The Pennsylvania State University. His research investigates the role of technological affordances in shaping psychological responses to a wide variety of mediated communications. He edited the Handbook of the Psychology of Communication Technology (Wiley, 2015) and currently serves as editor-in-chief of the Journal of Computer-Mediated Communication.


Copyright of Journalism & Mass Communication Quarterly is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use.